# Catalyst Index docker-compose defaults
OPENAI_API_KEY=your-openai-key
COHERE_API_KEY=your-cohere-key

CATALYST_ENVIRONMENT=dev
CATALYST_SECURITY__jwt_secret=dev-secret
CATALYST_STORAGE__vector_backend=qdrant
CATALYST_STORAGE__qdrant__host=qdrant
CATALYST_STORAGE__qdrant__port=6333
# Hybrid search (dense + sparse vectors) - leverages qodex-parse metadata (keywords, search_terms)
CATALYST_STORAGE__qdrant__sparse_vectors=true
CATALYST_STORAGE__term_index_backend=redis
CATALYST_STORAGE__redis__url=redis://redis:6379/0
CATALYST_STORAGE__artifacts__backend=local
CATALYST_STORAGE__artifacts__base_path=/data/artifacts
# For S3 artifact storage (set backend=s3 and configure these):
# CATALYST_STORAGE__artifacts__s3__bucket=catalyst-index-uploads
# CATALYST_STORAGE__artifacts__s3__prefix=artifacts
# CATALYST_STORAGE__artifacts__s3__region=us-east-1
CATALYST_STORAGE__vector_dimension=3072

CATALYST_EMBEDDINGS__provider=openai
CATALYST_EMBEDDINGS__model=text-embedding-3-large
CATALYST_EMBEDDINGS__allow_hash_fallback=false

CATALYST_RERANKER__enabled=true
CATALYST_RERANKER__provider=openai
CATALYST_RERANKER__model=text-embedding-3-large
CATALYST_RERANKER__weight=0.35

# Semantic Cache (2025 best practices for RAG cost optimization)
# Reduces LLM costs by 50-70% and improves response times by 4-15x
# Uses Redis DB /2 (separate from term index at /0 and job store at /1)
CATALYST_SEMANTIC_CACHE__enabled=true
CATALYST_SEMANTIC_CACHE__redis_url=redis://redis:6379/2
CATALYST_SEMANTIC_CACHE__distance_threshold=0.15  # Lower=stricter (0.1-0.15), higher=lenient (0.2-0.3)
CATALYST_SEMANTIC_CACHE__ttl_seconds=86400        # 24 hours default for RAG responses

CATALYST_POLICY_ADVISOR__enabled=true
CATALYST_POLICY_ADVISOR__model=gpt-4o-mini
CATALYST_POLICY_ADVISOR__api_key=${OPENAI_API_KEY}
CATALYST_POLICY_SYNTHESIS__enabled=true
CATALYST_POLICY_SYNTHESIS__model=gpt-4o-mini
CATALYST_POLICY_SYNTHESIS__api_key=${OPENAI_API_KEY}

CATALYST_JOBS__store__postgres_dsn=postgresql://catalyst:catalyst@postgres:5432/catalystjobs
CATALYST_JOBS__store__redis_url=redis://redis:6379/1
CATALYST_JOBS__worker__enabled=true
CATALYST_JOBS__worker__queue_name=ingestion
CATALYST_JOBS__worker__default_timeout=900
CATALYST_JOBS__worker__max_retries=3
CATALYST_JOBS__worker__retry_intervals=[15,30,60,120]
CATALYST_JOBS__worker__max_active_docs=4
CATALYST_JOBS__worker__max_queue_length=200

# LLM Enrichment Parallel Processing (NVIDIA RAG Best Practices)
CATALYST_JOBS__worker__llm_batch_size=8          # Chunks per batch (recommended: 8-16)
CATALYST_JOBS__worker__llm_max_workers=6         # Parallel batch workers (recommended: 3-6, None=auto)
CATALYST_JOBS__worker__llm_max_retries=2         # Retry attempts per chunk (recommended: 2)
CATALYST_JOBS__worker__llm_retry_delay_base=0.5  # Base delay (seconds) for exponential backoff
CATALYST_JOBS__worker__llm_max_concurrent_per_batch=8  # Max concurrent LLM calls within batch (None=batch_size, recommended: 4-8 for rate limiting)

CATALYST_FEATURES__enable_metrics=true
CATALYST_METRICS_EXPORTER_PORT=9464
CATALYST_METRICS_EXPORTER_ADDRESS=0.0.0.0

CATALYST_ACQUISITION__firecrawl__enabled=false
CATALYST_ACQUISITION__firecrawl__api_key=
